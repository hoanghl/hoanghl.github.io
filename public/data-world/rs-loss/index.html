<!doctype html><html lang=en><head data-base-url=https://hoanghl.io/ data-build-search-index=true data-comments-enabled=true data-giscus-category=General data-giscus-category-id=DIC_kwDOLQ4cp84CdIjK data-giscus-crossorigin=anonymous data-giscus-emit-metadata=0 data-giscus-input-position=bottom data-giscus-lang=en data-giscus-mapping=url data-giscus-nonce data-giscus-reactions-enabled=0 data-giscus-repo=kuznetsov17/pico data-giscus-repo-id=R_kgDOLQ4cpw data-giscus-src=https://giscus.app/client.js data-giscus-strict=0><meta charset=utf-8><title>Recommender system loss functions</title><script src=https://cdnjs.cloudflare.com/ajax/libs/slideout/1.0.1/slideout.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/slideout/1.0.1/slideout.min.js></script><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.css integrity=sha384-o3WH+1yEhq+grOgz1BVYTZPyTlMXrDxnjN1By9/ba94JqJhva6wFm2Hb+URQX53v rel=stylesheet><script crossorigin defer integrity=sha384-C5yZTsgLOfuizO9kb+hrB8uSBwwvZ4yenKWU0KmWl+7bkL6Tph/KbcOa3S4zdoRE src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.js></script><script crossorigin defer integrity=sha384-zWYbd0NBwgTsgIdFKVprSfTh1mbMPe5Hz1X3yY4Sd1h/K1cQoUe36OGwAGz/PcDy src=https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/mathtex-script-type.min.js></script><script crossorigin defer integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh onload=renderMathInElement(document.body); src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js></script><link href=https://hoanghl.io/site.css rel=stylesheet><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.css integrity=sha384-o3WH+1yEhq+grOgz1BVYTZPyTlMXrDxnjN1By9/ba94JqJhva6wFm2Hb+URQX53v rel=stylesheet><meta content="Some Description" name=description><meta content="Hoang Le" name=author><meta content=data,machine-learning,recommender-system,loss-function name=keywords><link href="https://hoanghl.io/css/grid.css?v=29062025103816" rel=stylesheet><link href="https://hoanghl.io/css/style.css?v=29062025103816" rel=stylesheet><link href=https://hoanghl.io/images/letter.jpg rel=icon type=image/png><link href=https://hoanghl.io/images/letter.jpg rel=mask-icon type=image/png><script src="https://hoanghl.io/js/codecopy.js?v=29062025103816"></script><script src="https://hoanghl.io/js/mermaid.min.js?v=29062025103816"></script><script src="https://hoanghl.io/elasticlunr.min.js?v=29062025103816"></script><script src="https://hoanghl.io/js/search.js?v=29062025103816"></script><body><header><div class=container id=header-container><div class=row><div class=col-2></div><div class=col-8 id=top><div id=menu><ul><li><a href=https://hoanghl.io/>home</a><li><a href=https://hoanghl.io/programming>programming</a><li><a class=active href=https://hoanghl.io/data-world>data-world</a></ul></div><div id=searchBox><span class="icon icon-search head"></span><input id=searchInput name=search></div><span class="icon head" id=cIcon></span></div><div class=col-2></div></div></div></header><div class=row><div class=col-12><div id=sResults><ul class=search-results id=sResultsUL></ul></div></div></div><main><div class=container id=main-container><div class=row><div class=col-2></div><div class="col-8 content"><h1 class=center>Recommender system loss functions</h1><span class=post-tags> <ul><li><a class=singlepost-tags href=https://hoanghl.io/tags/ml/>#ml</a> <li><a class=singlepost-tags href=https://hoanghl.io/tags/recommender-system/>#recommender-system</a> </ul> </span><div><details><summary><span class=toc-header>Contents</span></summary> <ul class=toc><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#0-brief-introduction>0. Brief introduction</a><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#1-as-next-word-prediction>1. As next word prediction</a><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#2-as-binary-classification-task>2. As binary classification task</a><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#3-as-ranking-problem>3. As ranking problem</a> <ul class=toc><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#3-1-bpr-loss>3.1. BPR loss</a><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#3-2-hinge-loss>3.2. Hinge loss</a><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#3-3-softmax-loss>3.3. Softmax loss</a><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#3-4-sampled-softmax-loss>3.4. Sampled Softmax loss</a><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#3-5-soft-nearest-neighbor>3.5. Soft nearest neighbor </a></ul><li class=toc><a href=https://hoanghl.io/data-world/rs-loss/#4-as-regression-problem>4. As regression problem</a></ul></details><h1 id=0-brief-introduction>0. Brief introduction</h1><p>This section gives a short introduction about the terms mentioned in the document.<p>Recommender system, is a very basic form, is a system that facilitates the users in selecting the suitable items among thousands of possible options in online platforms, such as e-commerce, social network, etc. In such system, the user picks and consumes an item (can be a movie, song, product). The action of consuming items is so-called <strong>interactions</strong>.<p>Regarding the data type, there are two popular data types in recommender system. Explicit feedback refers to the data type in which not only we know which user interacted with which item but also the level of preference for each interaction is explicit. On the other hand, implicit feedback is a data type whose the preference level is occluded. For example, in Netflix platform, each movie is attached the average rating from the users. That rating is calculated from the rating the users give to that movie. However, in reality, this data type isn’t popular and collecting it is neither cost-effective since not all users are willing to give a rate for a product/item. Therefore, the other data type, implicit feedback type, is more prevalent.<h1 id=1-as-next-word-prediction>1. As next word prediction</h1><ul><li>The problem is framed as next item prediction as next word prediction problem in NLP<li>The model generates the logits vector over item set. The item set may be the entire item set of the dataset or limited to the testing items only.<li>The loss is CrossEntropyLoss</ul><h1 id=2-as-binary-classification-task>2. As binary classification task</h1><ul><li>BCE loss/log loss<li>The interaction between user \( u \) and item \( i \) has value 1 if user \( u \) interacts item \( i \) , 0 otherwise ⇒ This consideration assumes that every unobserved interactions as negative one<li>The negative interactions are sampled from unobserved interactions</ul><p>Denote:<ul><li>User: \( u \)<li>Item: \( i \)<li>Target score: \( y_{ui} \) - probability value indicating whether user \( u \) interacts with item \( i \)<li>Predicted score: \( \hat{y}_{ui} \) - predicted probability<li>Interaction set \( \mathcal{Y} \)<li>Negative Interaction set \( \mathcal{Y}^- \) - set of (sampled) negative interactions</ul><script type="math/tex;mode=display">\mathcal{L} = - \Sigma_{(u, i) \in \mathcal{Y} \; \cup \; \mathcal{Y}^- } \Big( y_{ui} \log \big( \hat{y}_{ui} \big) + (1 - y_{ui}) \log (1 - \hat{y}_{ui}) \Big)</script><h1 id=3-as-ranking-problem>3. As ranking problem</h1><ul><li>target: a loss dedicated for crafting a list of items tailored for a specific user</ul><p>Denote:<ul><li>Sigmoid function: \( \sigma(x) = \dfrac{1}{1 + \exp (-x)} \)<li>The score predicted by the model for a specific user - item pair: \( \hat{y}_{ui} \)<li>Margin: \( m \)<li>Set of triplets (user \( u \), positive item \( i \), negative item \( j \) ):</ul><script type="math/tex;mode=display">D_S = \lbrace (u, i, j) \vert i \in I_u^+ \land j \in I_u^{-} \rbrace</script><h2 id=3-1-bpr-loss>3.1. BPR loss</h2><ul><li>traditional methods use interactions as positive and non-interaction as negative and likely combine with regularization<li>categorized as <strong>pairwise ranking loss</strong></ul><script type="math/tex;mode=display">\mathcal{L} = \Sigma_{u, i, j \in D} \ln \sigma(\hat{y}_{ui} - \hat{y}_{uj}) - \lambda_{\Theta} \lVert \Theta \rVert^2</script><h2 id=3-2-hinge-loss>3.2. Hinge loss</h2><ul><li>categorized as <strong>pairwise ranking loss</strong></ul><script type="math/tex;mode=display">\mathcal{L} = \max \big( m - \hat{y}_{ui} + \hat{y}_{uj}, 0 \big)</script><h2 id=3-3-softmax-loss>3.3. Softmax loss</h2><ul><li>categorized as <strong>listwise ranking loss</strong></ul><script type="math/tex;mode=display">\mathcal{L} = -\dfrac{1}{|\mathcal{D}|} \sum_{(u, i) \in \mathcal{D}} \log \Bigg\lbrace \dfrac{\exp (f(u, i))}{\sum_{j=1}^{\mathfrak{R}} \exp (f(u, j))} \Bigg\rbrace</script><p>where<ul><li>\( \mathcal{D} \): set of training data (containing tuple of user - positive item)<li>\( f(u, i) \): scoring function (estimated by neural network)<li>\( \mathfrak{R} \) is set of total items</ul><p>⇒ In other words, Softmax loss calculation is done over the entire set of items<p>One important point is that the function \( f(\cdot) \) indicates multiple things. In some works, it implies a function calculating the score given the user \( u \) and item \( i \) . In other works, however, it specifically implies the affinity score function given the user embedding and user embedding. This affinity function can be cosine similarity of dot product. Additionally, the latter implication refers to the last step of the former.<p>In conclusion, in either way of implication, the output is the score of the user \( u \) and item \( i \) .<h2 id=3-4-sampled-softmax-loss>3.4. Sampled Softmax loss</h2><p>Despite helping the model learn the most, the biggest issue of Softmax loss is the calculation is done over the entire set of the items, which make it infeasible in reality when the computation is prohibitively expensive, and the item set always change. One trick to tackle this problem is to do softmax over a subset of items. In particular, the Sample Softmax Loss is calculated as follow:</p><script type="math/tex;mode=display">\mathcal{L} = -\dfrac{1}{|\mathcal{D}|} \sum_{(u, i) \in \mathcal{D}} \log \Bigg\lbrace \dfrac{\exp (f(u, i))}{\exp (f(u, i)) + \sum_{j \in \mathcal{N}} \exp (f(u, j))} \Bigg\rbrace</script><p>where:<ul><li>\( \mathcal{N} \): set of negative items (dedicated for each user)</ul><h2 id=3-5-soft-nearest-neighbor>3.5. Soft nearest neighbor <a class=ref-link href=#bibref-frosst2019>[1]</a></h2><p>When dealing with multiple positive samples, one can employ the loss in contrastive learning.<p>Given:<ul><li>a batch \( b \) containing a list of samples \( {(\mathrm{x}_i, y_i) | i \in 1..b } \) where \( \mathrm{x}_i \in \mathbb{R}^n \) and \( y_i \) are correspondingly the hidden representation and categorical information of sample \( i \).<li>temperature \( \tau \in (0, 1] \)<li>\( d(\mathrm{x}_i, \mathrm{x}_j) \) is the distance defined upon 2 vectors \(\mathrm{x}_i \) \(\mathrm{x}_j \). It can be L2 or cosine similarity</ul><p>The soft nearest neighbor loss is defined as follow.</p><script type="math/tex;mode=display">\mathcal{L} = -\dfrac{1}{b} \sum_{i \in 1..b} \log \Bigg\lbrace \dfrac{\sum_{\substack{j \in 1..b \\ j \ne i \\ y_j = y_i}} \exp - \dfrac{d(\mathrm{x}_i, \mathrm{x}_j)}{\tau} }{\sum_{\substack{k \in 1..b \\ j \ne i}} \exp - \dfrac{d(\mathrm{x}_i, \mathrm{x}_k)}{\tau} } \Bigg\rbrace</script><h1 id=4-as-regression-problem>4. As regression problem</h1><ul><li>Treating recommendation task as regression problem is the earliest approach for recommender system.<li>this view isn’t suitable with implicit data since in this kind of data, the exact interested degree of a user toward a specific item is not explicitly available. Instead, we only know whether the user interacts with the item<li>Use MSELoss</ul><script type="math/tex;mode=display">\mathcal{L} = \dfrac{1}{N} \sum_{i=1}^N \big( \hat{y}_{ui} - y_{ui} \big)^2</script><div id=references><h1>References</h1><dl><dt class=ref-num id=bibref-frosst2019>[1]<dd>Nicholas Frosst and Nicolas Papernot and Geoffrey Hinton, Analyzing and Improving Representations with the Soft Nearest Neighbor Loss, 2019, <a href=https://arxiv.org/abs/1902.01889 target=_blank>https://arxiv.org/abs/1902.01889</a></dl></div></div></div><div class=col-2></div></div><div class="row postnav"><div class=col-2></div><div class=col-8><div class="col-6 left"><a class=postnav href=https://hoanghl.io/data-world/rs-eval-metrics/>← Evaluation metrics in recommender system</a></div><div class="col-6 right"></div></div><div class=col-2></div></div></div><div class=row><div class=col-2></div><div class=col-8 id=giscusWidget></div></div></main><script src="https://hoanghl.io/js/colortheme.js?v=29062025103816"></script><script src="https://hoanghl.io/js/init.js?v=29062025103816"></script>